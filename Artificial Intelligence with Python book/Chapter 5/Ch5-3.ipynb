{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an interactive object tracker using the CAMShift algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color space based tracking allows us to track colored objects, but we have to define the color first. This seems restrictive! Let us see how we can select an object in a live video and then have a tracker that can track it. This is where the CAMShift algorithm, which stands for Continuously Adaptive Mean Shift, becomes relevant. This is basically an adaptive version of the Mean Shift algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand CAMShift, let's see how Mean Shift works. Consider a region of interest in a given frame. We have selected this region because it contains the object of interest. We want to track this object, so we have drawn a rough boundary around it, which is what \"region of interest\" refers to. We want our object tracker to track this object as it moves around in the video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we select a set of points based on the color histogram of that region and then compute the centroid. If the location of this centroid is at the geometric center of this region, then we know that the object hasn't moved. But if the location of the centroid is not at the geometric center of this region, then we know that the object has moved. This means that we need to move the enclosing boundary as well. The movement of the centroid is directly indicative of the direction of movement of the object. We need to move our bounding box so that the new centroid becomes the geometric center of this bounding box. We keep doing this for every frame, and track the object in real time. Hence, this algorithm is called Mean Shift because the mean (i.e. the centroid) keeps shifting and we track the object using this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how this is related to CAMShift. One of the problems with Mean Shift is that the size of the object is not allowed to change over time. Once we draw a bounding box, it will stay constant regardless of how close or far away the object is from the camera. __This is why we need to use CAMShift because it can adapt the size of the bounding box to the size of the object.__ If you want to explore it further, you can check out this link: http://docs.opencv.org/3.1.0/db/df8/tutorial_py_meanshift.html . Let us see how to build a tracker.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class to handle all the functionality related to object tracking:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to handle object tracking related functionality\n",
    "class ObjectTracker(object):\n",
    "    def __init__(self, scaling_factor=0.5):\n",
    "        # Initialize the video capture object\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "\n",
    "        # Capture the frame from the webcam\n",
    "        _, self.frame = self.cap.read()\n",
    "\n",
    "        # Scaling factor for the captured frame\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "        # Resize the frame\n",
    "        self.frame = cv2.resize(self.frame, None, \n",
    "                fx=self.scaling_factor, fy=self.scaling_factor, \n",
    "                interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Create a window to display the frame\n",
    "        cv2.namedWindow('Object Tracker')\n",
    "\n",
    "        # Set the mouse callback function to track the mouse\n",
    "        cv2.setMouseCallback('Object Tracker', self.mouse_event)\n",
    "\n",
    "        # Initialize variable related to rectangular region selection\n",
    "        self.selection = None\n",
    "\n",
    "        # Initialize variable related to starting position \n",
    "        self.drag_start = None\n",
    "\n",
    "        # Initialize variable related to the state of tracking \n",
    "        self.tracking_state = 0\n",
    "\n",
    "    # Define a method to track the mouse events\n",
    "    def mouse_event(self, event, x, y, flags, param):\n",
    "        # Convert x and y coordinates into 16-bit numpy integers\n",
    "        x, y = np.int16([x, y]) \n",
    "\n",
    "        # Check if a mouse button down event has occurred\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drag_start = (x, y)\n",
    "            self.tracking_state = 0\n",
    "\n",
    "        # Check if the user has started selecting the region\n",
    "        if self.drag_start:\n",
    "            if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "                # Extract the dimensions of the frame\n",
    "                h, w = self.frame.shape[:2]\n",
    "\n",
    "                # Get the initial position\n",
    "                xi, yi = self.drag_start\n",
    "\n",
    "                # Get the max and min values\n",
    "                x0, y0 = np.maximum(0, np.minimum([xi, yi], [x, y]))\n",
    "                x1, y1 = np.minimum([w, h], np.maximum([xi, yi], [x, y]))\n",
    "\n",
    "                # Reset the selection variable\n",
    "                self.selection = None\n",
    "\n",
    "                # Finalize the rectangular selection\n",
    "                if x1-x0 > 0 and y1-y0 > 0:\n",
    "                    self.selection = (x0, y0, x1, y1)\n",
    "\n",
    "            else:\n",
    "                # If the selection is done, start tracking  \n",
    "                self.drag_start = None\n",
    "                if self.selection is not None:\n",
    "                    self.tracking_state = 1\n",
    "\n",
    "    # Method to start tracking the object\n",
    "    def start_tracking(self):\n",
    "        # Iterate until the user presses the Esc key\n",
    "        while True:\n",
    "            # Capture the frame from webcam\n",
    "            _, self.frame = self.cap.read()\n",
    "            \n",
    "            # Resize the input frame\n",
    "            self.frame = cv2.resize(self.frame, None, \n",
    "                    fx=self.scaling_factor, fy=self.scaling_factor, \n",
    "                    interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Create a copy of the frame\n",
    "            vis = self.frame.copy()\n",
    "\n",
    "            # Convert the frame to HSV colorspace\n",
    "            hsv = cv2.cvtColor(self.frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            # Create the mask based on predefined thresholds\n",
    "            mask = cv2.inRange(hsv, np.array((0., 60., 32.)), \n",
    "                        np.array((180., 255., 255.)))\n",
    "\n",
    "            # Check if the user has selected the region\n",
    "            if self.selection:\n",
    "                # Extract the coordinates of the selected rectangle\n",
    "                x0, y0, x1, y1 = self.selection\n",
    "\n",
    "                # Extract the tracking window\n",
    "                self.track_window = (x0, y0, x1-x0, y1-y0)\n",
    "\n",
    "                # Extract the regions of interest \n",
    "                hsv_roi = hsv[y0:y1, x0:x1]\n",
    "                mask_roi = mask[y0:y1, x0:x1]\n",
    "\n",
    "                # Compute the histogram of the region of \n",
    "                # interest in the HSV image using the mask\n",
    "                hist = cv2.calcHist( [hsv_roi], [0], mask_roi, \n",
    "                        [16], [0, 180] )\n",
    "\n",
    "                # Normalize and reshape the histogram\n",
    "                cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX);\n",
    "                self.hist = hist.reshape(-1)\n",
    "\n",
    "                # Extract the region of interest from the frame\n",
    "                vis_roi = vis[y0:y1, x0:x1]\n",
    "\n",
    "                # Compute the image negative (for display only)\n",
    "                cv2.bitwise_not(vis_roi, vis_roi)\n",
    "                vis[mask == 0] = 0\n",
    "\n",
    "            # Check if the system in the \"tracking\" mode\n",
    "            if self.tracking_state == 1:\n",
    "                # Reset the selection variable\n",
    "                self.selection = None\n",
    "                \n",
    "                # Compute the histogram back projection\n",
    "                hsv_backproj = cv2.calcBackProject([hsv], [0], \n",
    "                        self.hist, [0, 180], 1)\n",
    "\n",
    "                # Compute bitwise AND between histogram \n",
    "                # backprojection and the mask\n",
    "                hsv_backproj &= mask\n",
    "\n",
    "                # Define termination criteria for the tracker\n",
    "                term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, \n",
    "                        10, 1)\n",
    "\n",
    "                # Apply CAMShift on 'hsv_backproj'\n",
    "                track_box, self.track_window = cv2.CamShift(hsv_backproj, \n",
    "                        self.track_window, term_crit)\n",
    "\n",
    "                # Draw an ellipse around the object\n",
    "                cv2.ellipse(vis, track_box, (0, 255, 0), 2)\n",
    "\n",
    "            # Show the output live video\n",
    "            cv2.imshow('Object Tracker', vis)\n",
    "\n",
    "            # Stop if the user hits the 'Esc' key\n",
    "            c = cv2.waitKey(5)\n",
    "            if c == 27:\n",
    "                break\n",
    "\n",
    "        # Close all the windows\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObjectTracker().start_tracking() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
