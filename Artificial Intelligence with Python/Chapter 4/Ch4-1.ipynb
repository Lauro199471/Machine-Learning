{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Machine-learning systems are usually built using different modules.__ These modules are combined in a particular way to achieve an end goal. __The scikit-learn library has functions that enable us to build these pipelines by concatenating various modules together.__ We just need to specify the modules along with the corresponding parameters. It will then build a pipeline using these modules that processes the data and trains the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline can include modules that perform various functions like feature selection, preprocessing, random forests, clustering, and so on. In this section, we will see how to build a pipeline to select the top K features from an input data point and then classify them using an Extremely Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import samples_generator \n",
    "from sklearn.feature_selection import SelectKBest, f_regression \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the line to follow, we create 150 data points, where each data point is a 25-dimensional feature vector. The numbers in each feature vector will be generated using a random sample generator. Each data point has 6 informative features and no redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:  (150, 25)\n",
      "X example: \n",
      " [ 1.01856035 -0.1850947   0.33953529  0.88377939 -2.22145741 -0.71205954\n",
      "  0.46313981 -2.42424476 -0.07998485  0.03653191 -1.27561144 -1.5670243\n",
      " -0.82216114 -0.47040384  0.98701872 -0.34439804  0.02056176 -1.65437764\n",
      "  0.94696772 -0.22854693  0.40599781  0.16376894 -0.89722827  2.43356744\n",
      " -0.69119524]\n",
      "\n",
      "Y Shape:  (150,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Generate data  \n",
    "X, y = samples_generator.make_classification(n_samples=150,  \n",
    "        n_features=25, n_classes=3, n_informative=6,  \n",
    "        n_redundant=0, random_state=7) \n",
    "print(\"X Shape: \" , np.shape(X))\n",
    "print(\"X example: \\n\" , X[0])\n",
    "\n",
    "print(\"\\nY Shape: \" , np.shape(y))\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first block in the pipeline is the feature selector. This block selects the K best features. Let's set the value of K to 9, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top K features  \n",
    "k_best_selector = SelectKBest(f_regression, k=9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block in the pipeline is an Extremely Random Forests classifier with 60 estimators and a maximum depth of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Extremely Random Forests classifier  \n",
    "classifier = ExtraTreesClassifier(n_estimators=60, max_depth=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the pipeline by joining the individual blocks that we've constructed. We can name each block so that it's easier to track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the pipeline \n",
    "processor_pipeline = Pipeline([('selector', k_best_selector), ('erf', classifier)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the parameters of the individual blocks. Let's change the value of K in the first block to 7 and the number of estimators in the second block to 30. We will use the names we assigned in the previous line to define the scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('selector', SelectKBest(k=7, score_func=<function f_regression at 0x0000021531103950>)), ('erf', ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_spl...ators=30, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the parameters \n",
    "processor_pipeline.set_params(selector__k=7, erf__n_estimators=30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the pipeline using the sample data that we generated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('selector', SelectKBest(k=7, score_func=<function f_regression at 0x0000021531103950>)), ('erf', ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_spl...ators=30, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the pipeline  \n",
    "processor_pipeline.fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the output for all the input values and print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted output:\n",
      " [1 2 2 0 2 0 2 1 0 1 1 2 2 0 2 2 1 0 0 1 0 2 1 1 2 2 0 0 1 2 1 2 1 0 2 2 1\n",
      " 1 2 2 2 0 1 2 2 1 2 2 1 0 1 2 2 2 2 0 2 2 0 2 2 0 1 0 2 2 1 1 1 2 0 1 0 2\n",
      " 0 0 1 2 2 0 0 2 2 2 2 0 0 0 2 2 2 1 2 0 2 2 2 2 0 0 1 1 1 1 2 2 1 2 1 1 1\n",
      " 0 2 1 1 0 1 1 1 1 0 0 0 1 2 0 0 0 2 1 2 0 0 1 1 1 1 0 1 1 1 2 0 2 0 1 2 0\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Predict outputs for the input data \n",
    "output = processor_pipeline.predict(X) \n",
    "print(\"\\nPredicted output:\\n\", output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual output:\n",
      " [0 2 2 0 2 0 2 1 0 1 1 2 1 0 2 2 1 0 0 1 0 1 0 1 2 2 0 0 1 0 1 2 1 0 2 2 1\n",
      " 1 2 2 2 0 0 0 2 1 1 2 1 0 1 2 2 1 2 0 2 2 0 2 2 0 1 0 2 1 1 1 1 2 0 1 0 2\n",
      " 0 0 1 2 2 0 0 1 0 2 2 0 0 0 2 2 2 1 2 0 2 0 2 0 0 0 1 1 1 1 2 2 2 2 0 1 1\n",
      " 0 2 1 1 0 1 1 1 1 0 0 0 1 2 0 0 0 2 1 2 0 0 1 0 1 1 0 1 1 1 1 2 2 0 1 1 0\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nActual output:\\n\" , y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the score using the labeled training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Print scores  \n",
    "print(\"\\nScore:\", processor_pipeline.score(X, y)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features chosen by the selector block. We specified that we wanted to choose 7 features out of 25. Use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indices of selected features: 4, 7, 8, 12, 14, 17, 22\n"
     ]
    }
   ],
   "source": [
    "# Print the features chosen by the pipeline selector \n",
    "status = processor_pipeline.named_steps['selector'].get_support() \n",
    " \n",
    "# Extract and print indices of selected features \n",
    "selected = [i for i, x in enumerate(status) if x] \n",
    "print(\"\\nIndices of selected features:\", ', '.join([str(x) for x in selected])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted output list in the preceding screenshot shows the output labels predicted using the processor. The score represents the effectiveness of the processor. The last line indicates the indices of the chosen features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
