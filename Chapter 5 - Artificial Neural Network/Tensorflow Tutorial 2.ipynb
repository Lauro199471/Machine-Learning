{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first make TensorFlow perform our little example calculation:\n",
    "\n",
    "a=(b+c)∗(c+2)  \n",
    "____________\n",
    "d = b + c\n",
    "\n",
    "e = c + 2\n",
    "\n",
    "a = d * e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a TensorFlow constant\n",
    "const = tf.constant(2.0, name=\"const\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TensorFlow variables\n",
    "b = tf.Variable(2.0, name='b')\n",
    "c = tf.Variable(1.0, name='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create some operations\n",
    "d = tf.add(b, c, name='d')\n",
    "e = tf.add(c, const, name='e')\n",
    "a = tf.multiply(d, e, name='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to setup an object to initialise the variables and the graph structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the variable initialisation\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow session is an object where all operations are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is 9.0\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    # compute the output of the graph\n",
    "    a_out = sess.run(a)\n",
    "    print(\"Variable a is {}\".format(a_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TensorFlow placeholder\n",
    "Let’s also say that we didn’t know what the value of the array b would be during the declaration phase of the TensorFlow problem (i.e. before the with tf.Session() as sess) stage.  In this case, TensorFlow requires us to declare the basic structure of the data by using the tf.placeholder variable declaration.  Let’s use it for b:\n",
    "\n",
    "Because we aren’t providing an initialisation in this declaration, we need to tell TensorFlow what data type each element within the tensor is going to be.  In this case, we want to use tf.float32. The second argument is the shape of the data that will be “injected” into this variable.  In this case, we want to use a (? x 1) sized array – because we are being cagey about how much data we are supplying to this variable (hence the “?”), the placeholder is willing to accept a None argument in the size declaration.  Now we can inject as much 1-dimensional data that we want into the b variable.\n",
    "\n",
    "Here we remove the mystery and specify exactly what the variable b is to be – a one-dimensional range from 0 to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TensorFlow variables\n",
    "b = tf.placeholder(tf.float32, [None, 1], name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable a is 9.0\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    # compute the output of the graph\n",
    "    a_out = sess.run(a, feed_dict={b: np.arange(0, 10)[:, np.newaxis]})\n",
    "    print(\"Variable a is {}\".format(a_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Network Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MNIST dataset is a set of 28×28 pixel grayscale images which represent hand-written digits.  It has 55,000 training rows, 10,000 testing rows and 5,000 validation rows.\n",
    "\n",
    "We can load the data by running:\n",
    "\n",
    "The one_hot=True argument specifies that instead of the labels associated with each image being the digit itself i.e. “4”, it is a vector with “one hot” node and all the other nodes being zero i.e. [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].  This lets us easily feed it into the output layer of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-8bf8ae5a5303>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Lauro Cabral\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Lauro Cabral\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Lauro Cabral\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Lauro Cabral\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Lauro Cabral\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python optimisation variables\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to setup the weight and bias variables for the three layer neural network.  There are always L-1 number of weights/bias tensors, where L is the number of layers.  So in this case, we need to setup two tensors for each:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This neural network will have 300 nodes in the hidden layer, so the size of the weight tensor W1 is [784, 300].  We initialise the values of the weights using a random normal distribution with a mean of zero and a standard deviation of 0.03. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([300]), name='b1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we create W2 and b2 variables to connect the hidden layer to the output layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to setup node inputs and activation functions of the hidden layer nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "# output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting the output y_ to a clipped version, limited between 0.000001 to 0.999999.  This is to make sure that we never get a case were we have a log(0) operation occurring during training – this would return NaN and break the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy calculation\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                         + (1 - y) * tf.log(1 - y_clipped), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let’s setup the optimiser in TensorFlow:\n",
    "\n",
    "Here we are just using the gradient descent optimiser provided by TensorFlow.  We initialize it with a learning rate, then specify what we want it to do – i.e. minimise the cross entropy cost operation we created.  This function will then perform the gradient descent (for more details on gradient descent see here and here) and the backpropagation for you.  How easy is that?  TensorFlow has a library of popular neural network training optimisers, see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now have everything we need to setup the training process of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 2.090\n",
      "Epoch: 2 cost = 0.904\n",
      "Epoch: 3 cost = 0.694\n",
      "Epoch: 4 cost = 0.620\n",
      "Epoch: 5 cost = 0.578\n",
      "Epoch: 6 cost = 0.551\n",
      "Epoch: 7 cost = 0.531\n",
      "Epoch: 8 cost = 0.514\n",
      "Epoch: 9 cost = 0.499\n",
      "Epoch: 10 cost = 0.486\n",
      "Epoch: 11 cost = 0.474\n",
      "Epoch: 12 cost = 0.462\n",
      "Epoch: 13 cost = 0.450\n",
      "Epoch: 14 cost = 0.439\n",
      "Epoch: 15 cost = 0.428\n",
      "Epoch: 16 cost = 0.416\n",
      "Epoch: 17 cost = 0.405\n",
      "Epoch: 18 cost = 0.395\n",
      "Epoch: 19 cost = 0.385\n",
      "Epoch: 20 cost = 0.375\n",
      "Epoch: 21 cost = 0.365\n",
      "Epoch: 22 cost = 0.356\n",
      "Epoch: 23 cost = 0.347\n",
      "Epoch: 24 cost = 0.338\n",
      "Epoch: 25 cost = 0.329\n",
      "Epoch: 26 cost = 0.321\n",
      "Epoch: 27 cost = 0.313\n",
      "Epoch: 28 cost = 0.306\n",
      "Epoch: 29 cost = 0.298\n",
      "Epoch: 30 cost = 0.291\n",
      "Epoch: 31 cost = 0.284\n",
      "Epoch: 32 cost = 0.278\n",
      "Epoch: 33 cost = 0.272\n",
      "Epoch: 34 cost = 0.266\n",
      "Epoch: 35 cost = 0.260\n",
      "Epoch: 36 cost = 0.254\n",
      "Epoch: 37 cost = 0.249\n",
      "Epoch: 38 cost = 0.243\n",
      "Epoch: 39 cost = 0.239\n",
      "Epoch: 40 cost = 0.234\n",
      "Epoch: 41 cost = 0.229\n",
      "Epoch: 42 cost = 0.224\n",
      "Epoch: 43 cost = 0.220\n",
      "Epoch: 44 cost = 0.216\n",
      "Epoch: 45 cost = 0.212\n",
      "Epoch: 46 cost = 0.208\n",
      "Epoch: 47 cost = 0.204\n",
      "Epoch: 48 cost = 0.201\n",
      "Epoch: 49 cost = 0.197\n",
      "Epoch: 50 cost = 0.193\n",
      "Epoch: 51 cost = 0.190\n",
      "Epoch: 52 cost = 0.187\n",
      "Epoch: 53 cost = 0.184\n",
      "Epoch: 54 cost = 0.181\n",
      "Epoch: 55 cost = 0.178\n",
      "Epoch: 56 cost = 0.175\n",
      "Epoch: 57 cost = 0.172\n",
      "Epoch: 58 cost = 0.169\n",
      "Epoch: 59 cost = 0.167\n",
      "Epoch: 60 cost = 0.164\n",
      "Epoch: 61 cost = 0.162\n",
      "Epoch: 62 cost = 0.159\n",
      "Epoch: 63 cost = 0.157\n",
      "Epoch: 64 cost = 0.154\n",
      "Epoch: 65 cost = 0.152\n",
      "Epoch: 66 cost = 0.150\n",
      "Epoch: 67 cost = 0.148\n",
      "Epoch: 68 cost = 0.146\n",
      "Epoch: 69 cost = 0.144\n",
      "Epoch: 70 cost = 0.142\n",
      "Epoch: 71 cost = 0.140\n",
      "Epoch: 72 cost = 0.138\n",
      "Epoch: 73 cost = 0.136\n",
      "Epoch: 74 cost = 0.134\n",
      "Epoch: 75 cost = 0.132\n",
      "Epoch: 76 cost = 0.131\n",
      "Epoch: 77 cost = 0.129\n",
      "Epoch: 78 cost = 0.128\n",
      "Epoch: 79 cost = 0.126\n",
      "Epoch: 80 cost = 0.124\n",
      "Epoch: 81 cost = 0.123\n",
      "Epoch: 82 cost = 0.121\n",
      "Epoch: 83 cost = 0.120\n",
      "Epoch: 84 cost = 0.118\n",
      "Epoch: 85 cost = 0.117\n",
      "Epoch: 86 cost = 0.116\n",
      "Epoch: 87 cost = 0.114\n",
      "Epoch: 88 cost = 0.113\n",
      "Epoch: 89 cost = 0.112\n",
      "Epoch: 90 cost = 0.110\n",
      "Epoch: 91 cost = 0.109\n",
      "Epoch: 92 cost = 0.108\n",
      "Epoch: 93 cost = 0.107\n",
      "Epoch: 94 cost = 0.105\n",
      "Epoch: 95 cost = 0.104\n",
      "Epoch: 96 cost = 0.103\n",
      "Epoch: 97 cost = 0.102\n",
      "Epoch: 98 cost = 0.101\n",
      "Epoch: 99 cost = 0.100\n",
      "Epoch: 100 cost = 0.098\n",
      "0.9757\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    # initialise the variables\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy],feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
